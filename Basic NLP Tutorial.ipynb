{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome Basic NLP Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application : Gender Classification by Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load library\n",
    "import nltk as nlp\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>_golden</th>\n",
       "      <th>_unit_state</th>\n",
       "      <th>_trusted_judgments</th>\n",
       "      <th>_last_judgment_at</th>\n",
       "      <th>gender</th>\n",
       "      <th>gender:confidence</th>\n",
       "      <th>profile_yn</th>\n",
       "      <th>profile_yn:confidence</th>\n",
       "      <th>created</th>\n",
       "      <th>...</th>\n",
       "      <th>profileimage</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>sidebar_color</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_count</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>815719226</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>10/26/15 23:24</td>\n",
       "      <td>male</td>\n",
       "      <td>1.0</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12/5/13 1:48</td>\n",
       "      <td>...</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/414342229...</td>\n",
       "      <td>0</td>\n",
       "      <td>FFFFFF</td>\n",
       "      <td>Robbie E Responds To Critics After Win Against...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>110964</td>\n",
       "      <td>10/26/15 12:40</td>\n",
       "      <td>6.587300e+17</td>\n",
       "      <td>main; @Kan1shk3</td>\n",
       "      <td>Chennai</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    _unit_id  _golden _unit_state  _trusted_judgments _last_judgment_at  \\\n",
       "0  815719226    False   finalized                   3    10/26/15 23:24   \n",
       "\n",
       "  gender  gender:confidence profile_yn  profile_yn:confidence       created  \\\n",
       "0   male                1.0        yes                    1.0  12/5/13 1:48   \n",
       "\n",
       "       ...                                            profileimage  \\\n",
       "0      ...       https://pbs.twimg.com/profile_images/414342229...   \n",
       "\n",
       "   retweet_count sidebar_color  \\\n",
       "0              0        FFFFFF   \n",
       "\n",
       "                                                text tweet_coord tweet_count  \\\n",
       "0  Robbie E Responds To Critics After Win Against...         NaN      110964   \n",
       "\n",
       "    tweet_created      tweet_id   tweet_location user_timezone  \n",
       "0  10/26/15 12:40  6.587300e+17  main; @Kan1shk3       Chennai  \n",
       "\n",
       "[1 rows x 26 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import data\n",
    "dataset = pd.read_csv(r\"gender-classifier.csv\", encoding=\"latin1\")\n",
    "dataset.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['_unit_id', '_golden', '_unit_state', '_trusted_judgments',\n",
       "       '_last_judgment_at', 'gender', 'gender:confidence', 'profile_yn',\n",
       "       'profile_yn:confidence', 'created', 'description', 'fav_number',\n",
       "       'gender_gold', 'link_color', 'name', 'profile_yn_gold', 'profileimage',\n",
       "       'retweet_count', 'sidebar_color', 'text', 'tweet_coord', 'tweet_count',\n",
       "       'tweet_created', 'tweet_id', 'tweet_location', 'user_timezone'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>male</td>\n",
       "      <td>i sing my own rhythm.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>male</td>\n",
       "      <td>I'm the author of novels filled with family dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>male</td>\n",
       "      <td>louis whining and squealing and all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>male</td>\n",
       "      <td>Mobile guy.  49ers, Shazam, Google, Kleiner Pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>female</td>\n",
       "      <td>Ricky Wilson The Best FRONTMAN/Kaiser Chiefs T...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender                                        description\n",
       "0    male                              i sing my own rhythm.\n",
       "1    male  I'm the author of novels filled with family dr...\n",
       "2    male                louis whining and squealing and all\n",
       "3    male  Mobile guy.  49ers, Shazam, Google, Kleiner Pe...\n",
       "4  female  Ricky Wilson The Best FRONTMAN/Kaiser Chiefs T..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get gender and description\n",
    "dataset = dataset [[\"gender\",\"description\"]]\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nan values count: 3744\n",
      "Cleaned\n"
     ]
    }
   ],
   "source": [
    "#check and clean nan values\n",
    "print(\"Nan values count:\",dataset.description.isna().sum())\n",
    "dataset.dropna(axis=0,inplace=True)\n",
    "print(\"Cleaned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO\n",
    "* Transform Gender Feature \n",
    "* Select a sample for example\n",
    "* Clean data from smile,point vs..\n",
    "* Word tokenize\n",
    "* Clean data from stopwords\n",
    "* Get words root (lemmatization)\n",
    "* Join words\n",
    "* All process implement all description\n",
    "* Bag Of Words\n",
    "* Train-Test Split\n",
    "* Search Random Forest's Best Pramaeters\n",
    "* Classification\n",
    "* Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>i sing my own rhythm.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender            description\n",
       "0       1  i sing my own rhythm."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tranform gender from str to int\n",
    "dataset.gender = [1 if i==\"male\" else 0 for i in dataset.gender]\n",
    "dataset.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description: A global marketplace for images, videos and music. Sharing photos, inspiration, design tips & videos for the creative community. word count: 19\n"
     ]
    }
   ],
   "source": [
    "#Select a sample for example\n",
    "sample = dataset.loc[6,:]\n",
    "print(\"Description:\",sample.description,\"word count:\",len(sample.description.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description:  A global marketplace for images  videos and music  Sharing photos  inspiration  design tips   videos for the creative community \n"
     ]
    }
   ],
   "source": [
    "#Clean data from smile,point vs..\n",
    "import re #for regular expression\n",
    "sample.description = re.sub(\"[^a-zA-z]\", \" \",sample.description) \n",
    "print(\"Description: \",sample.description )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " 'global',\n",
       " 'marketplace',\n",
       " 'for',\n",
       " 'images',\n",
       " 'videos',\n",
       " 'and',\n",
       " 'music',\n",
       " 'Sharing',\n",
       " 'photos',\n",
       " 'inspiration',\n",
       " 'design',\n",
       " 'tips',\n",
       " 'videos',\n",
       " 'for',\n",
       " 'the',\n",
       " 'creative',\n",
       " 'community']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import nltk\n",
    "#nltk.download('punkt')\n",
    "\n",
    "#Word tokenize\n",
    "sample.description = nlp.word_tokenize(sample.description) #split words: He didn't homework : \"he\",\"did\",\"not\",\"homework\"\n",
    "sample.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " 'global',\n",
       " 'marketplace',\n",
       " 'images',\n",
       " 'videos',\n",
       " 'music',\n",
       " 'Sharing',\n",
       " 'photos',\n",
       " 'inspiration',\n",
       " 'design',\n",
       " 'tips',\n",
       " 'videos',\n",
       " 'creative',\n",
       " 'community']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.download('stopwords')\n",
    "\n",
    "#Clean data from stopwords\n",
    "from nltk.corpus import stopwords\n",
    "sample.description = [i for i in sample.description if not i in set(stopwords.words(\"english\"))]\n",
    "sample.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " 'global',\n",
       " 'marketplace',\n",
       " 'image',\n",
       " 'video',\n",
       " 'music',\n",
       " 'Sharing',\n",
       " 'photo',\n",
       " 'inspiration',\n",
       " 'design',\n",
       " 'tip',\n",
       " 'video',\n",
       " 'creative',\n",
       " 'community']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.download('wordnet')\n",
    "\n",
    "#Get words root (lemmatization) example: videos -> video, tips -> tip\n",
    "\n",
    "lemma = nlp.WordNetLemmatizer()\n",
    "sample.description = [lemma.lemmatize(i) for  i in sample.description]\n",
    "sample.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A global marketplace image video music Sharing photo inspiration design tip video creative community'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.description = \" \".join(sample.description)\n",
    "sample.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                 i sing my own rhythm\n",
       "1    I m the author of novel filled with family dra...\n",
       "2                  louis whining and squealing and all\n",
       "3    Mobile guy er Shazam Google Kleiner Perkins Ya...\n",
       "4    Ricky Wilson The Best FRONTMAN Kaiser Chiefs T...\n",
       "5                                    you don t know me\n",
       "6    A global marketplace for image video and music...\n",
       "7       The secret of getting ahead is getting started\n",
       "8                 Pll Fan Crazy about MCD Ramen is bae\n",
       "9    Renaissance art historian University of Nottin...\n",
       "Name: description, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#All process implement all description\n",
    "\n",
    "def preprocess(x):\n",
    "    x = str(x)\n",
    "    x = re.sub(\"[^a-zA-z]\", \" \",x)\n",
    "    x = nlp.word_tokenize(x)\n",
    "    #x = [i for i in x if not i in set(stopwords.words(\"english\"))] #slowly\n",
    "    x = [lemma.lemmatize(i) for  i in x]\n",
    "    x = \" \".join(x)\n",
    "    return x\n",
    "\n",
    "dataset.description = dataset.description.apply(preprocess)\n",
    "dataset.description[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En sÄ±k kullanÄ±lan kelimeler: ['__', '___', '_n', 'account', 'action', 'activist', 'actor', 'actress', 'add', 'addict', 'addicted', 'adult', 'adventure', 'advertising', 'advice', 'advocate', 'affiliate', 'aficionado', 'african', 'age', 'agency', 'ain', 'air', 'aka', 'alive', 'alternative', 'alum', 'amateur', 'amazing', 'ambassador', 'america', 'american', 'analysis', 'analyst', 'android', 'angel', 'animal', 'anime', 'answer', 'anti', 'app', 'apple', 'area', 'arsenal', 'art', 'artist', 'ask', 'aspiring', 'assistant', 'association', 'atheist', 'athlete', 'athletics', 'author', 'available', 'average', 'avid', 'award', 'away', 'awesome', 'baby', 'bad', 'bae', 'ball', 'band', 'bar', 'baseball', 'based', 'basketball', 'beach', 'bear', 'beat', 'beautiful', 'beauty', 'beer', 'believe', 'believer', 'best', 'better', 'bi', 'bieber', 'big', 'biggest', 'bio', 'bit', 'bitch', 'black', 'blacklivesmatter', 'blessed', 'blind', 'blog', 'blogger', 'blue', 'board', 'body', 'book', 'booking', 'bookings', 'books', 'born', 'bot', 'boxing', 'boy', 'brand', 'breaking', 'bring', 'bringing', 'british', 'broken', 'brother', 'build', 'building', 'business', 'business_inquiry', 'buy', 'ca', 'cab', 'cake', 'california', 'called', 'canada', 'canadian', 'cancer', 'captain', 'car', 'card', 'care', 'career', 'cat', 'cause', 'celebrity', 'center', 'ceo', 'certified', 'champion', 'change', 'channel', 'character', 'chat', 'check', 'chelsea', 'chicago', 'chief', 'child', 'chocolate', 'choice', 'christ', 'christian', 'church', 'city', 'class', 'classic', 'click', 'client', 'cloud', 'club', 'coach', 'coast', 'coffee', 'collector', 'college', 'com', 'come', 'comedian', 'comic', 'coming', 'commercial', 'communication', 'community', 'company', 'computer', 'connect', 'conservative', 'consultant', 'contact', 'content', 'continuous', 'contributor', 'control', 'cook', 'cool', 'country', 'county', 'course', 'cover', 'coverage', 'covering', 'craft', 'crazy', 'create', 'created', 'creating', 'creative', 'creator', 'credit', 'cricket', 'culture', 'current', 'currently', 'customer', 'cute', 'cyclist', 'da', 'dad', 'daddy', 'daily', 'damn', 'dan', 'dance', 'dancer', 'dark', 'data', 'date', 'daughter', 'day', 'dc', 'dead', 'deal', 'death', 'dedicated', 'deep', 'design', 'designer', 'developer', 'development', 'did', 'didn', 'die', 'different', 'digital', 'direction', 'director', 'discover', 'disney', 'dj', 'dm', 'doe', 'doesn', 'dog', 'doing', 'don', 'dont', 'draw', 'dream', 'dreamer', 'drink', 'dude', 'earth', 'east', 'easy', 'eat', 'ed', 'editor', 'education', 'el', 'email', 'employer', 'empowered', 'en', 'end', 'endorsement', 'engineer', 'england', 'english', 'enjoy', 'entertainment', 'enthusiast', 'entrepreneur', 'er', 'especially', 'est', 'estate', 'et', 'event', 'events', 'everybody', 'everyday', 'ex', 'executive', 'experience', 'expert', 'expressed', 'extremely', 'eye', 'face', 'facebook', 'faith', 'fall', 'family', 'fan', 'fanatic', 'fangirl', 'fans', 'fantasy', 'fashion', 'fast', 'father', 'favorite', 'favourite', 'fb', 'fc', 'fear', 'feed', 'feel', 'female', 'feminist', 'fi', 'fiction', 'fight', 'fighting', 'film', 'finance', 'financial', 'financially', 'finding', 'fine', 'firm', 'fitness', 'flash', 'florida', 'fly', 'fm', 'focus', 'follow', 'followed', 'follower', 'following', 'follows', 'food', 'foodie', 'football', 'forever', 'forget', 'founder', 'freak', 'free', 'freedom', 'freelance', 'fresh', 'friend', 'friendly', 'friends', 'fuck', 'fucking', 'fun', 'funny', 'future', 'game', 'gamer', 'games', 'gaming', 'gay', 'geek', 'general', 'geo', 'geography', 'getting', 'gift', 'girl', 'girls', 'global', 'gmail', 'goal', 'god', 'going', 'gold', 'golf', 'gon', 'good', 'google', 'got', 'government', 'grad', 'graduate', 'graphic', 'great', 'greatest', 'green', 'grfoxfjwpv', 'group', 'grow', 'growing', 'guide', 'guitar', 'guy', 'gym', 'ha', 'hair', 'half', 'hand', 'happen', 'happiness', 'happy', 'hard', 'harry', 'hate', 'having', 'head', 'health', 'healthcare', 'healthy', 'heart', 'hell', 'hello', 'help', 'helping', 'hero', 'hey', 'hi', 'high', 'highly', 'hip', 'history', 'hit', 'hockey', 'holder', 'home', 'hop', 'hope', 'host', 'hot', 'hottest', 'hour', 'house', 'houston', 'http', 'huge', 'human', 'humble', 'husband', 'icon', 'idea', 'ig', 'im', 'important', 'improve', 'independent', 'indie', 'individual', 'industry', 'info', 'information', 'innovation', 'inspiration', 'inspirational', 'insta', 'instagram', 'interested', 'interesting', 'international', 'internet', 'investment', 'iphone', 'irish', 'isn', 'issue', 'item', 'itunes', 'jack', 'james', 'jesus', 'job', 'john', 'join', 'joke', 'journalist', 'junior', 'junkie', 'just', 'justice', 'justin', 'kath', 'keeping', 'kid', 'kind', 'king', 'know', 'knowledge', 'known', 'la', 'lady', 'largest', 'latest', 'laugh', 'law', 'lawyer', 'le', 'lead', 'leader', 'leadership', 'leading', 'league', 'learn', 'learning', 'leave', 'left', 'legal', 'let', 'level', 'liberal', 'library', 'life', 'lifestyle', 'light', 'like', 'lil', 'line', 'link', 'listen', 'little', 'live', 'living', 'll', 'local', 'lol', 'london', 'long', 'look', 'looking', 'lord', 'los', 'lost', 'lot', 'louis', 'love', 'loved', 'lover', 'loves', 'loving', 'ma', 'mad', 'magazine', 'mail', 'major', 'make', 'maker', 'makeup', 'making', 'male', 'man', 'management', 'manager', 'managing', 'manchester', 'map', 'maps', 'market', 'marketer', 'marketing', 'married', 'master', 'matter', 'maybe', 'mean', 'media', 'medical', 'medium', 'meet', 'member', 'men', 'met', 'metal', 'mi', 'miami', 'michael', 'military', 'mind', 'minute', 'miss', 'mission', 'mix', 'mobile', 'model', 'modern', 'mom', 'moment', 'mommy', 'mon', 'money', 'moon', 'morning', 'mother', 'movie', 'movies', 'mufc', 'mum', 'music', 'musician', 'na', 'nation', 'national', 'native', 'natural', 'nature', 'nba', 'nc', 'nd', 'need', 'nerd', 'network', 'new', 'news', 'nfl', 'nice', 'nigeria', 'nigga', 'night', 'non', 'north', 'novel', 'nsfw', 'number', 'ny', 'nyc', 'obsessed', 'oc', 'occasional', 'offer', 'office', 'officer', 'official', 'oh', 'ohio', 'old', 'online', 'open', 'opinion', 'opinions', 'opportunity', 'order', 'organization', 'original', 'outlook', 'owner', 'page', 'pain', 'paper', 'parent', 'partner', 'party', 'passion', 'passionate', 'pc', 'peace', 'people', 'perfect', 'person', 'personal', 'pet', 'phd', 'phone', 'photo', 'photographer', 'photography', 'pic', 'picture', 'piece', 'pin', 'pizza', 'place', 'plan', 'play', 'player', 'playing', 'plus', 'pm', 'podcast', 'poet', 'policy', 'political', 'politics', 'pop', 'porn', 'positive', 'post', 'power', 'pr', 'practice', 'president', 'press', 'pretty', 'price', 'prince', 'princess', 'private', 'pro', 'probably', 'problem', 'producer', 'product', 'prof', 'professional', 'professor', 'profile', 'profit', 'program', 'progress', 'progressive', 'project', 'promo', 'promote', 'promoting', 'promotion', 'proud', 'provide', 'provides', 'providing', 'ps', 'public', 'published', 'publisher', 'purpose', 'quality', 'que', 'queen', 'question', 'quiz', 'quote', 'radio', 'raised', 'random', 'rap', 'read', 'reader', 'reading', 'ready', 'real', 'reality', 'really', 'reason', 'record', 'red', 'register', 'related', 'remember', 'rendezvous', 'reporter', 'research', 'resident', 'resource', 'rest', 'restaurant', 'retired', 'retweets', 'review', 'right', 'rip', 'road', 'rock', 'romance', 'rp', 'rt', 'rts', 'rule', 'run', 'runner', 'running', 'rzoj', 'sad', 'said', 'sale', 'sales', 'san', 'save', 'say', 'sc', 'school', 'sci', 'science', 'se', 'search', 'season', 'seattle', 'second', 'secret', 'secure', 'security', 'seen', 'self', 'sell', 'selling', 'send', 'senior', 'sense', 'seo', 'series', 'service', 'serving', 'set', 'sex', 'sexy', 'shall', 'share', 'sharing', 'shit', 'shop', 'shopping', 'short', 'sign', 'simple', 'singer', 'single', 'sister', 'site', 'sleep', 'small', 'smile', 'snap', 'snapchat', 'soccer', 'social', 'socialmedia', 'society', 'software', 'solution', 'son', 'song', 'songwriter', 'soon', 'sorry', 'soul', 'sound', 'source', 'south', 'southern', 'space', 'speaker', 'special', 'specialist', 'specializing', 'spirit', 'spiritually', 'sport', 'sports', 'st', 'stan', 'stand', 'star', 'start', 'started', 'startup', 'state', 'station', 'stay', 'step', 'stock', 'stop', 'store', 'story', 'straight', 'strategist', 'strategy', 'stream', 'streamer', 'street', 'strength', 'strong', 'student', 'stuff', 'style', 'subscribe', 'success', 'sun', 'super', 'support', 'supporter', 'supporting', 'sure', 'sweet', 'ta', 'taken', 'taking', 'talent', 'talk', 'talkradio', 'targeted', 'tea', 'teacher', 'team', 'tech', 'technology', 'tell', 'tennis', 'texas', 'th', 'thank', 'thanks', 'theatre', 'thing', 'think', 'thinking', 'thought', 'time', 'tip', 'today', 'tour', 'town', 'trade', 'trainer', 'training', 'trash', 'travel', 'traveler', 'traveller', 'true', 'trust', 'truth', 'try', 'trying', 'turn', 'tv', 'tweet', 'tweeting', 'tweetmyjobs', 'tweets', 'twitch', 'twitter', 'type', 'uk', 'unique', 'united', 'universe', 'university', 'update', 'ur', 'urban', 'usa', 'use', 'using', 've', 'vegan', 'veteran', 'video', 'videos', 'view', 'views', 'vintage', 'visit', 'voice', 'volunteer', 'wa', 'wait', 'walk', 'walking', 'wan', 'want', 'war', 'warrior', 'washington', 'watch', 'watching', 'water', 'way', 'wear', 'weather', 'web', 'website', 'week', 'weird', 'welcome', 'west', 'white', 'wife', 'wild', 'win', 'wine', 'winner', 'winning', 'wish', 'wolf', 'woman', 'won', 'word', 'work', 'worker', 'working', 'world', 'worldwide', 'worth', 'wrestling', 'write', 'writer', 'writing', 'wrong', 'wwe', 'xbox', 'xx', 'ya', 'yeah', 'year', 'yes', 'yo', 'yoga', 'york', 'young', 'youth', 'youtube', 'youtuber', 'yr', 'zayn']\n"
     ]
    }
   ],
   "source": [
    "#Bag Of Words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "max_features = 1000\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=max_features, stop_words = \"english\")\n",
    "sparce_matrix = vectorizer.fit_transform(dataset.description.values.astype('U')).toarray()\n",
    "\n",
    "print(\"En sÄ±k kullanÄ±lan kelimeler:\",vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : (14601, 1000)\n",
      "Test  : (1623, 1000)\n"
     ]
    }
   ],
   "source": [
    "#Train-Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = sparce_matrix\n",
    "y = dataset.gender\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.1,random_state=42)\n",
    "print(\"Train :\",X_train.shape)\n",
    "print(\"Test  :\",X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.model_selection import GridSearchCV\\nparam_grid = [{\\'n_estimators\\':[10,50,100],\\n               \\'max_depth\\': np.arange(1, 10),\\n               \\'min_samples_leaf\\': [1, 5, 20, 50, 100],\\n               \\'min_weight_fraction_leaf\\': [0.0,0.1,0.3],\\n               \\'random_state\\':[1,4,7]}]\\ngridCV = GridSearchCV(estimator=rf, param_grid=param_grid,cv=10)\\ngridCV = gridCV.fit(X_train, y_train)\\n\\nn_est = gridCV.best_params_[\"n_estimators\"]\\nmd = gridCV.best_params_[\"max_depth\"]\\nmsl = gridCV.best_params_[\"min_samples_leaf\"]\\nmwfl = gridCV.best_params_[\"min_weight_fraction_leaf\"]\\nrs = gridCV.best_params_[\"random_state\"]\\nprint(gridCV.best_score_)\\nprint(gridCV.best_params_)\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Search Random Forest's Best Pramaeters\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier()\n",
    "\"\"\"\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = [{'n_estimators':[10,50,100],\n",
    "               'max_depth': np.arange(1, 10),\n",
    "               'min_samples_leaf': [1, 5, 20, 50, 100],\n",
    "               'min_weight_fraction_leaf': [0.0,0.1,0.3],\n",
    "               'random_state':[1,4,7]}]\n",
    "gridCV = GridSearchCV(estimator=rf, param_grid=param_grid,cv=10)\n",
    "gridCV = gridCV.fit(X_train, y_train)\n",
    "\n",
    "n_est = gridCV.best_params_[\"n_estimators\"]\n",
    "md = gridCV.best_params_[\"max_depth\"]\n",
    "msl = gridCV.best_params_[\"min_samples_leaf\"]\n",
    "mwfl = gridCV.best_params_[\"min_weight_fraction_leaf\"]\n",
    "rs = gridCV.best_params_[\"random_state\"]\n",
    "print(gridCV.best_score_)\n",
    "print(gridCV.best_params_)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification\n",
    "clf = RandomForestClassifier()#n_estimators=n_est, max_depth=md, min_samples_leaf=msl, min_weight_fraction_leaf=mwfl,random_state=rs\n",
    "clf.fit(X_train, y_train)\n",
    "pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.67775723968\n",
      "F1 Score : 0.422099447514\n"
     ]
    }
   ],
   "source": [
    "#Result\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "acc = accuracy_score(y_test, pred)\n",
    "f1 = f1_score(y_test, pred)\n",
    "\n",
    "print(\"Accuracy :\",acc)\n",
    "print(\"F1 Score :\",f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
